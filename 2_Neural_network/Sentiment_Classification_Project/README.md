- ### What You Should Already Know

  - neural networks, forward and back-propagation
  - stochastic gradient descent
  - mean squared error
  - and train/test splits


  ### Tutorial Outline:

  - Intro: The Importance of "Framing a Problem" (this lesson)

  - [Curate a Dataset](#lesson_1)
  - [Developing a "Predictive Theory"](#lesson_2)
  - [**PROJECT 1**: Quick Theory Validation](#project_1)


  - [Transforming Text to Numbers](#lesson_3)
  - [**PROJECT 2**: Creating the Input/Output Data](#project_2)


  - Putting it all together in a Neural Network 
  - [**PROJECT 3**: Building our Neural Network](#project_3)


  - [Understanding Neural Noise](#lesson_4)
  - [**PROJECT 4**: Making Learning Faster by Reducing Noise](#project_4)


  - [Analyzing Inefficiencies in our Network](#lesson_5)
  - [**PROJECT 5**: Making our Network Train and Run Faster](#project_5)


  - [Further Noise Reduction](#lesson_6)
  - [**PROJECT 6**: Reducing Noise by Strategically Reducing the Vocabulary](#project_6)


  - [Analysis: What's going on in the weights?](#lesson_7)
  
 
 
 ### Where to Get Help if You Need it
  - Re-watch previous Udacity Lectures
  - Leverage the recommended Course Reading Material - [Grokking Deep Learning](https://www.manning.com/books/grokking-deep-learning) by Andrew Trask
  ref- Udacity_Deep_Learning_NanoDegree

  